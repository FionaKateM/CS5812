# Data Management Plan

## Overview

**Researchers:** Fiona Morgan, ...
**Project title:** tbc
**Project duration:** 12 weeks
**Project context**: CS5812 Group Coursework - Machine Learning & Deep Learning

## Defining your data/research sources

**2.1 Where will your data/research sources come from?**
To begin our research, we decided to use a dataset called "the 2000s Movie Database". The dataset is described by developer Claudia Cifaldi as part of Kaggle's "The Movies Dataset", which is available on https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset. The chosen dataset is available in the following GitHub repository, https://github.com/cla-cif/movie-DB-2000s.

**2.2 How often will you get new data?**
We will not be getting new data.

**2.3 How much data/information will you generate?**
The dataset is about 400KB. No additional data will be generated.

**2.4 What file formats will you use?**
The data is stored in CSV format. Analysis and modelling will be completed in R-Studio and therefore Rmd or R file formats, or Google Colab. 

**3.1 How will you structure and name your folders and files?**
The read me in the GitHub will specify the file names of importance. The data files will all start with 'movies_' followed by a descriptive string based on the contents of the file and finally a version number.

**3.2 What additional information is required to understand each data file?**
As we are going to be comparing the success of multiple models we will need to keep the data used the same in each case, therefore the test / train split will be done as part of data preparation. 

**3.3 What different versions of each data file or source will your create?**
File names will contain a version number in the format 'v1.01' where the units are big changes and the decimals are smaller updates. 

**4.1  Where will you store your data?**
GitHub. 

**4.2 How will your data be backed up?**
All data in GitHub will also be present on the personal computers of others. 

**4.3 How will you test whether you can restore from your backups?**
All data stored in the GitHub repository will be downloaded and then tested in a different and new environment (such as a different computer).

**5.1 Who owns the data you generate?**
It is a publicly available dataset from kaggle with a Public Domain licence. 

**5.2 Who else has a right to see or use this data?**
It is a publicly available dataset from kaggle with a Public Domain licence. 

**5.3 Who else should reasonably have access to this data when you share it?**
It is a publicly available dataset from kaggle with a Public Domain licence. 

**5.4 What should/shouldn’t be shared and why?**
There is no part of this dataset that cannot be shared.

**6.1 What should be archived beyond the end of your project?**
The original dataset and the files that cleaned and prepared it. 

**6.2 For how long should it be stored?**
*EPSRC guidelines say “10 years from the date of last access”*

**6.3 When will files be moved into the data archive/repository?**
When the project is handed in. 

**6.4 Where will the data be stored?**
Personal computers and GitHub.

**6.5 Who is responsible for moving data to the data archive and maintaining it?**
Individuals on the project.

**6.6 Who should have access and under what conditions?**
There is no need for restrictions. 

**7.1 Who is responsible for making sure this plan is followed?**
Individuals on the project.

**7.2 How often will this plan be reviewed and updated?**
Regularly throughout the project - managed by Fiona Morgan. 

**7.3 What actions have you identified from the rest of this plan?**
Ensuring everyone on the project is comfortable with the use of GitHub and the naming conventions.

**7.4 What further information do you need to carry out these actions?**
No further information needed.
